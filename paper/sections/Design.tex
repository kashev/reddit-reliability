In this section, we show the design of our system. Additionally, we discuss some
of the challenges, limitations, and design decisions that went into making it.
Finally, we discuss some of the details of the implementation.

\subsection{Gathering \reddit{} Usernames} % (fold)
\label{sub:gathering_reddit_usernames}

The first limitation of the \reddit{} API is that usernames are an `open
secret'. If one has the username of an account, public information about that
account can be retrieved, but there is no way to directly get usernames.
Instead, what we were forced to do was scrape the posts of popular subreddits
and get the usernames of the author of every post and comment. In doing so, we
were able to collect over 150K usernames. Of the usernames we collected, we
randomly selected around 3K to fully gather data on and run our regression model
on.

One issue with this approach is that this makes it impossible to identify non-
participants. If a user never comments on a post, or posts a post themselves,
there is no way to know that that user exists. This is unfortunately an
insurmountable limitation. Instead, we chose only to find good and bad users to
train our classifier, and ignore non-participants.

% subsection gathering_reddit_usernames (end)

\subsection{\reddit{} API Limits} % (fold)
\label{sub:reddit_api_limits}

\reddit{} has an API limit of 30 requests per minute when one hasn't authorized
their script properly, and 60 requests per minute when authorized. Given our
timeframe, we eschewed the authorization method and decided to go with the less
time-efficient method of not authorizing our script. In gathering data, however,
we discovered this limit is not strictly enforced, but in order to be good
citizens and as to not get our access revoked, we knew we had to design around
these constraints. As such, in order to both speed up our ability to access user
data, dynamically change the features that we used (see
\Cref{sub:feature_selection}), and reduce load on \reddit{}'s servers, we used a
MongoDB instance to cache the data we were gathering. Not were we able to store
raw data from \reddit{} API calls, we were also able to cache results from more
computationally intensive features that we were generating.

% subsection reddit_api_limits (end)

\subsection{Establishing Ground Truth} % (fold)
\label{sub:establishing_ground_truth}
% will add more detail later. take a look at it, i wrote this at 2 am.
Establishing ground truth was done in two stages. The first stage was to find
reliable users. Finding these users was relatively trivial as the site rewards
positive behavior through karma, which leads to increased visibility. From there
the users could be filtered by their level of contribution manually. Moderators
from various communities were also taken for their work in helping the
community.  These users were used in our training set for a reliability score
$s_r = 1$.

The second stage was to find users who were unreliable or detrimental to the
community. Eventually, we discovered subreddits dedicated to weeding out users
that didn't contribute and various posts that detailed accounts that were used
to abuse the community. These were used as our training set for a reliability
score $s_r = -1$.

% subsection establishing_a_ground_truth (end)

\subsection{Feature Selection} % (fold)
\label{sub:feature_selection}

Before having a trained regression model, there was no way to tell what
characteristics of \reddit{} users would be important. However, one can't get
this regression model without these features. So, we decided to pick a wide
variety of features generated from the raw data, and figured out what was
important later by looking at our results. Here, we present features which we
suspected would be important. A discussion of what features were actually
important is in \Cref{sec:evaluation}.

\subsubsection{\reddit{} Karma and Derivatives} % (fold)
\label{ssub:reddit_karma_and_derivatives}

The most important and useful feature of the \reddit{} platform is the fact that
it has a built in voting system. We hypothesized that users whose posts and
comments are well received would have positive correlation with that users
reliability. However, not all content on \reddit{} is news-worthy. There are
many, many subreddits in which there is everything from pets to porn. Thus, we
established several sub-criteria in order to differentiate between karma earned
by posting reliable news and karma earned by posting picture of cats.

First, we broke down karma earned in the top 100, 50, 25, and 10 subreddits as
reported by redditlist \cite{redditlist}. This way that users who participate
mostly in small, niche subreddits would be somewhat penalized for not
participating in `useful' subreddits. However, it is possible that one would
want to know the reliability of users within a certain community. For the
purposes of this project, we picked a small list of `trusted' subreddits, and
picked good, reliable users from those communities as training examples. Then,
the amount of karma accrued from those subreddits was another feature that we
used.

% subsubsection reddit_karma_and_derivatives (end)

\subsubsection{\reddit{} Gold and Gilded Content} % (fold)
\label{ssub:reddit_gold_and_gilded_content}

Another interesting feature of \reddit{} which differentiates it from other
social networks is \reddit{} gold. \reddit{} gold is a premium membership which
users may pay for, and comes with many perks, like an ad-free experience, a
members only subreddit, deals from partner companies, and more
\cite{redditgold}. What makes \reddit{}'s gold program so unique is that users
are encouraged not only to buy this for themselves, but also to give \reddit{}
gold membership as a gift to other users who produce content that they enjoy.
Furthermore, this content is marked as `gilded', and can be tracked. We
suspected that this action would be extremely important in determining reliable
content, and only content which was reliable would be worth a user spending real
money to reward.

% subsubsection reddit_gold_and_gilded_content (end)


\subsubsection{Natural Language Processing} % (fold)
\label{ssub:natural_language_processing}

Finally, we suspected that the actual writing patterns of reliable users would
separate them from unreliable ones. Specifically, we tried three things that we
suspected might have some sort of correlation with reliability.

\begin{enumerate}
    \item \textbf{Unique Word Percentage}: We suspected that users who were
    reliable would use more unique words than those who were not.

    \item \textbf{Readability}: We analyzed the Flesch--Kincaid readability
    \cite{kincaid1975derivation} of the comments users posted, which can be
    calculated using the formula in \Cref{eq:fk_eq}.

    \begin{equation} \label{eq:fk_eq}
        206.835 - 1.015 \left(\frac{\text{\tiny \# words}}{\text{\tiny \# sentences}}\right) - 84.6 \left(\frac{\text{\tiny \# syllables}}{\text{\tiny \# words}}\right)
    \end{equation}

    \item \textbf{Swearing Rate}: We took a list of `bad' words which Google
    compiled \cite{googlebadwords}, and calculated the percentage of words that
    users used in comments which appeared on that list.

\end{enumerate}

% subsubsection natural_language_processing (end)

% subsection feature_selection (end)

\input{tables/feature_table.tex}

\subsection{Picking a Regression Model} % (fold)
\label{sub:picking_a_regression_model}

In picking a regression model, two things needed to be considered, namely: (1)
the robustness of the model, and (2) the ease with which we could garner
insights from the model generated.

Ultimately, random forests \cite{598994} was chosen based on the notion that the
results were more interpretable than other popular modeling algorithms such as
neural networks, and would provide greater reliability than just a simple
decision tree given that random forests has boosting built in.

% subsection picking_a_regression_model (end)
