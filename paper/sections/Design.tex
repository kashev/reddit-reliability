In this section, we show the design of our system, starting from data
gathering/aggregation, through to the final modeling/analysis of the data
obtained. Additionally, we discuss some of the challenges, limitations, and
design decisions that went into making it.  Finally, we discuss some of the
finer details of the system's implementation.

\subsection{Gathering \reddit{} Usernames} % (fold)
\label{sub:gathering_reddit_usernames}

The first limitation of the \reddit{} API is that usernames are an `open
secret'. If one has the username of an account, public information about that
account can be retrieved. However, there there is no way to directly get
usernames.  Instead, one has to gather usernames by scraping posts of subreddits
and getting the author's username of every post and comment. In doing so, we
were able to collect over 150K usernames. Of the usernames we collected, we
randomly selected around 3K to fully gather data on and run our regression model
on.

One issue with this approach is that this makes it impossible to identify non-
participants. If a user neither never comments on a post, nor posts a post
themselves, there is no way to know that that user exists. This is unfortunately
an insurmountable limitation given the existing API. Instead, we chose only to
find good and bad users who have demonstrated activity on \reddit{} to train our
classifier, and ignore non-participants.

% subsection gathering_reddit_usernames (end)

\subsection{\reddit{} API Limits} % (fold)
\label{sub:reddit_api_limits}

\reddit{} has an API limit of 30 requests per minute when one hasn't authorized
their script, and 60 requests per minute when authorized. Given our timeframe,
we eschewed the authorization method and decided to go with the less
time-efficient method of not authorizing our script. In gathering data, however,
we discovered this limit is not strictly enforced. In order to be good citizens
and gather the data we wanted whilst simultaneously working not get our access
revoked, we knew we had to design around these constraints. As such, to both
speed up our ability to access user data, dynamically change the features that
we used (see \Cref{sub:feature_selection}), and reduce load on \reddit{}'s
servers, we used a MongoDB instance to cache the data we were gathering. Not
were we able to store raw data from \reddit{} API calls, we were also able to
cache results from more computationally intensive features that we were
generating.

% subsection reddit_api_limits (end)

\subsection{Establishing Ground Truth} % (fold)
\label{sub:establishing_ground_truth}
% will add more detail later. take a look at it, i wrote this at 2 am.
Establishing ground truth was done in two stages, the first of which was to find
reliable users. Finding these users was relatively trivial as the site rewards
positive behavior through karma, leading to increased visibility. From there,
users could be filtered manually by their level of contribution. Moderators from
various communities were also included in this set for their work in the
\reddit{} community. These users were used in our training set and were given a
reliability score of $s_r = 1$.

The second stage was to find users who were unreliable or detrimental to the
community. Eventually, we discovered subreddits dedicated to contributing
content to \reddit{} that was either not useful or not constructive in promoting
\reddit{} as a reliable site, and numerous posts that suggested their authors
were accounts used to abuse the community. These were also used as part of our
training set, and were given a reliability score of $s_r = -1$.

% subsection establishing_a_ground_truth (end)

\subsection{Feature Selection} % (fold)
\label{sub:feature_selection}

Before having a trained regression model, there was no way to tell what
characteristics of \reddit{} users would be important. However, one can't get
this regression model without these features. So, we decided to pick a wide
variety of features generated from the raw data, and figured out what was
important later by looking at our results. Here, we present features which we
suspected would be important. A discussion of what features were actually
important is in \Cref{sec:evaluation}.

\subsubsection{\reddit{} Karma and Derivatives} % (fold)
\label{ssub:reddit_karma_and_derivatives}

Perhaps the most important feature of the \reddit{} platform, and one of the
reasons why \reddit{} is an incredibly relevant resource in today's changing
news landscape is its built in voting system. We hypothesized that users whose
posts and comments are well received would have positive correlation with that
users reliability. However, not all content on \reddit{} is news-worthy. There
are numerous subreddits in which the content is less than news-worthy,
containing things like cat pictures and pornography, to name a few topics.. Thus,
we established several sub-criteria in order to differentiate between karma
earned by posting reliable news and karma earned by posting content that merely
engaged the community's softer side, such as by posting picture of cats.

We broke down a user's net karma into karma earned in the top 100, 50, 25, and
10 subreddits as reported by redditlist \cite{redditlist}. This way, users who
participate mostly in small, niche subreddits would be somewhat penalized for
not participating in more mainstream, `useful' subreddits as part of a measure
of "impactfulness". For the purposes of this project, we picked a small list of
`trusted' subreddits, and picked good, reliable users from those communities as
training examples, and used a users' net karma from the top $n$ subreddits as
features in our classifier.
% subsubsection reddit_karma_and_derivatives (end)

\subsubsection{\reddit{} Gold and Gilded Content} % (fold)
\label{ssub:reddit_gold_and_gilded_content}

Another interesting feature of \reddit{} which differentiates it from other
social networks is \reddit{} gold. \reddit{} gold is a premium membership which
users may pay for. It comes with many perks, such as ad-free experience, a
members-only subreddit, and deals from partner companies, amongst a number of
other benefits \cite{redditgold}. What makes \reddit{}'s gold program so unique,
however, is that users are encouraged not only to buy it for themselves, but
also to give \reddit{} gold membership as a gift to other users who produce
content that they enjoy.

Content that motivates users to purchase \reddit{} gold for others is marked as
being `gilded', and is marked as being such. This action can be done numerous
times for any particular piece of content submitted/posted to \reddit{}, to the
point where a post can be gilded upwards of five or six times within the span of
a few hours. Given the impactfulness and meaning behind a user's being given
\reddit{} gold, we suspected that this action would be extremely important in
determining reliable content in that content which was reliable or incredibly
enjoyable/not detrimental to the well-being of the site would be worth a user
spending real money to reward.

% subsubsection reddit_gold_and_gilded_content (end)


\subsubsection{Natural Language Processing} % (fold)
\label{ssub:natural_language_processing}

Finally, we suspected that the actual writing patterns of reliable users would
separate them from unreliable ones. Specifically, we looked at three metrics that we
suspected might correlate with a user's reliability.

\begin{enumerate}
    \item \textbf{Unique Word Percentage}: We suspected that users who were
      reliable would use more unique words than those who were not.

    \item \textbf{Readability}: We analyzed the Flesch--Kincaid readability
      \cite{kincaid1975derivation} score of the comments users posted, which can
      be calculated using the formula in \Cref{eq:fk_eq}.

    \begin{equation} \label{eq:fk_eq}
        206.835 - 1.015 \left(\frac{\text{\tiny \# words}}{\text{\tiny \# sentences}}\right) - 84.6 \left(\frac{\text{\tiny \# syllables}}{\text{\tiny \# words}}\right)
    \end{equation}

    \item \textbf{Swearing Rate}: We took a list of `bad' words which Google
      compiled \cite{googlebadwords}, and calculated the percentage of those
      words that users used in comments.

\end{enumerate}

% subsubsection natural_language_processing (end)

% subsection feature_selection (end)

\input{tables/feature_table.tex}

\subsection{Picking a Regression Model} % (fold)
\label{sub:picking_a_regression_model}

In picking a regression model, two things needed to be considered, namely: (1)
the robustness of the model, and (2) the ease with which we could garner
insights from the model generated.

Ultimately, random forests \cite{598994} was chosen based on the notion that the
results were more interpretable than other popular modeling algorithms such as
neural networks, and would provide greater reliability than just a simple
decision tree.

% subsection picking_a_regression_model (end)
