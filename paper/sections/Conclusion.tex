In this section, we discuss some weaknesses in our project and discuss future
improvements that may be made.

\subsection{Weaknesses}
\label{sub:weaknesses}
Given the relatively unexplored landscape and the restrictions of the \reddit{}
API, there exist many weaknesses in our analysis of \reddit{}.

First and foremost, a larger pool of users need to be analysed in order to get a
better, more complete picture of what \reddit{} is truly like. 3,200 users is an
incredibly small subset of the number of users \reddit{} has, and does not paint
a full picture of how reliable the site is as a whole. A less biased pool of
users would also be beneficial in trying to come up with a more robust model; in
training, we artificially took our data and converted it from its natural bias
of roughly 90\% unreliable users into a 50\% bias in order to allow our model to
actually learn useful features. Having access to a pool of usernames that were
deemed as being active and reliable would have helped immensely in decreasing
the inherent bias and in turn, would have accelerated the data
gathering/analysis process.

Secondly, more signals could be included in our classifier. Due to time
constraints, there were features we would have liked to add into our classifier
that we were unable to include, such as a temporal analysis of a user's
reliability factoring into how reliable they may be at any given time. It is
also worth noting that the data we have is solely limited to what \reddit{}
provides openly, and the model we have could be made more robust if greater
access to data (such as subreddit subscriptions, etc) were provided.

Third, the training dataset was created rather subjectively instead of being
created/scored by numerous individuals blindly. Given sufficient resources, the
training dataset would have ideally been created via the use of Amazon
Mechanical Turk, allowing for the crowdsourcing (and inherent cross-validation)
of the training dataset. It also would allow us to have a test set to work off
of, which we unfortunately did not have the resources to generate given the
difficulty involved in generating a set of reliable users.

\subsubsection{Conclusion}

Given the set of users we have obtained in conjunction with the variability that
exists among the various subsets of reddit users there exists weaknesses in our
analysis. However, given the current state of research of reddit and the
increasing reliance of individuals on reddit as a source of reliable information
our contribution noteworthy for the purpose of exploration with respect to
reddit as an up-and-coming source of information.

% subsection weaknesses (end)

\subsection{Future Work} % (fold)
\label{sub:future_work}
In addition to addressing the weaknesses presented previously, there exists
quite a bit of future work that can be tackled.

To begin, the problem of establishing ground truth is a rather difficult one,
and is one that we decided not to take on given the time constraints placed upon
us. There is no objective way to establish ground truth, and the level of
subjectivity required in establishing it would require not only greater temporal
resources, but also financial resources that we do not possess. Given that users
can be constructive and reliable in different ways, we chose perhaps the most
obvious route of identifying reliability, but there are more nuanced senses of
reliability that would help identify users as being reliable despite not being
``mainstream''.

The problem of voting on \reddit{} is also an open issue; redditors not only
vote because the content is important and reliable, but also because the content
is personally interesting to them. This means that it may be the case that an up
and coming, incredibly important news article submitted to \texttt{r/news} may
have the same amount of karma and be gilded the same way as a cute cat picture
on \texttt{r/aww}. Identifying the differences between these different types of
karma and finding a proper measurement/way to include them in our measurement of
reliability is rather important, and should be addressed in the future.

In trying to promote itself as an up and coming news site, \reddit{} also has a
feature called \reddit{} live, where redditors can share up-to-date information
on events as they occur via a live stream. Leveraging the information gathered
from \reddit{} live events, including the participants/individuals who submit
content to those live streams, the content submitted, etc. would help augment
our model and provide a better sense of reliability given the immediate nature
of the feature.

Finally, building a more robust system to do a lot of the work that we did
manually, and then extract likely true events from \reddit{}, would be extremely
useful. Work like this has been done for other real-time social networks like
Twitter and Instagram in Apollo \cite{Le:2011:DDL:2070942.2071018}.

% subsection future work(end)
